"use strict";(self.webpackChunksiglens_docs=self.webpackChunksiglens_docs||[]).push([[1873],{1561:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>r,default:()=>h,frontMatter:()=>i,metadata:()=>l,toc:()=>c});var t=s(5893),a=s(1151);const i={},r="Benchmark Against ClickHouse and Elasticsearch",l={id:"benchmarks/nyc-taxi-benchmark-runbook",title:"Benchmark Against ClickHouse and Elasticsearch",description:"Common setup",source:"@site/docs/benchmarks/nyc-taxi-benchmark-runbook.md",sourceDirName:"benchmarks",slug:"/benchmarks/nyc-taxi-benchmark-runbook",permalink:"/siglens-docs/benchmarks/nyc-taxi-benchmark-runbook",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{}},o={},c=[{value:"Common setup",id:"common-setup",level:2},{value:"Making the dataset",id:"making-the-dataset",level:2},{value:"Benchmark SigLens",id:"benchmark-siglens",level:2},{value:"Install Go",id:"install-go",level:3},{value:"Clone SigLens",id:"clone-siglens",level:3},{value:"Enable AgileAggs",id:"enable-agileaggs",level:3},{value:"Start SigLens",id:"start-siglens",level:3},{value:"Setup PQS",id:"setup-pqs",level:3},{value:"Setup an ingestion script",id:"setup-an-ingestion-script",level:3},{value:"Ingest the data into SigLens",id:"ingest-the-data-into-siglens",level:3},{value:"Restart SigLens",id:"restart-siglens",level:3},{value:"View Logs",id:"view-logs",level:3},{value:"Run the Queries in SigLens",id:"run-the-queries-in-siglens",level:3},{value:"Benchmark ClickHouse",id:"benchmark-clickhouse",level:2},{value:"Install ClickHouse",id:"install-clickhouse",level:3},{value:"Configure the ClickHouse data folder",id:"configure-the-clickhouse-data-folder",level:3},{value:"Run ClickHouse",id:"run-clickhouse",level:3},{value:"Make the ClickHouse Table",id:"make-the-clickhouse-table",level:3},{value:"Ingest the Data",id:"ingest-the-data",level:3},{value:"Run the Queries in ClickHouse",id:"run-the-queries-in-clickhouse",level:3},{value:"Benchmark Elasticsearch",id:"benchmark-elasticsearch",level:2},{value:"Download Elasticsearch",id:"download-elasticsearch",level:3},{value:"Configure Elasticsearch",id:"configure-elasticsearch",level:3},{value:"Add a template",id:"add-a-template",level:3},{value:"Save the password for later",id:"save-the-password-for-later",level:3},{value:"Run Elasticsearch",id:"run-elasticsearch",level:3},{value:"Ingest the Data",id:"ingest-the-data-1",level:3},{value:"Prepare to run queries",id:"prepare-to-run-queries",level:3},{value:"Run the Queries in Elasticsearch",id:"run-the-queries-in-elasticsearch",level:3},{value:"Benchmark Loki",id:"benchmark-loki",level:2},{value:"Install Loki and Promtail",id:"install-loki-and-promtail",level:3},{value:"Update the default configs",id:"update-the-default-configs",level:3},{value:"Run Loki and Promtail",id:"run-loki-and-promtail",level:3},{value:"Ingest the Data",id:"ingest-the-data-2",level:3},{value:"Run the Queries in Loki",id:"run-the-queries-in-loki",level:3}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",...(0,a.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"benchmark-against-clickhouse-and-elasticsearch",children:"Benchmark Against ClickHouse and Elasticsearch"}),"\n",(0,t.jsx)(n.h2,{id:"common-setup",children:"Common setup"}),"\n",(0,t.jsx)(n.p,{children:"Setup a server to run the benchmarks.\nI used an AWS im4gn.2xlarge running Ubuntu 22.04.\nThis instance has 8 vCPUs, 32 GB of RAM, and 3.5 TB of storage, but you have to mount the storage with the following steps:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"ssh into your server"}),"\n",(0,t.jsxs)(n.li,{children:["Run ",(0,t.jsx)(n.code,{children:"lsblk"})," and you should see something like the following, with the ",(0,t.jsx)(n.code,{children:"nvme1n1"})," item having 3.4 TB of storage."]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"NAME         MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS\nloop0          7:0    0  21.3M  1 loop /snap/amazon-ssm-agent/7529\nloop1          7:1    0  49.1M  1 loop /snap/core18/2794\nloop2          7:2    0  59.3M  1 loop /snap/core20/2019\nloop3          7:3    0 109.6M  1 loop /snap/lxd/24326\nloop4          7:4    0  35.5M  1 loop /snap/snapd/20102\nnvme0n1      259:0    0     8G  0 disk\n\u251c\u2500nvme0n1p1  259:2    0   7.9G  0 part /\n\u2514\u2500nvme0n1p15 259:3    0    99M  0 part /boot/efi\nnvme1n1      259:1    0   3.4T  0 disk\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Mount the storage"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sudo mkfs.xfs /dev/nvme1n1\nsudo mkdir /mnt/nvme1n1\nsudo mount /dev/nvme1n1 /mnt/nvme1n1\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"4",children:["\n",(0,t.jsxs)(n.li,{children:["You can check that it's mounted by running ",(0,t.jsx)(n.code,{children:"df -h"})," and you should see something like this:"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Filesystem       Size  Used Avail Use% Mounted on\n/dev/root        7.6G  1.5G  6.2G  20% /\ntmpfs             16G     0   16G   0% /dev/shm\ntmpfs            6.2G  948K  6.2G   1% /run\ntmpfs            5.0M     0  5.0M   0% /run/lock\n/dev/nvme0n1p15   98M  6.3M   92M   7% /boot/efi\ntmpfs            3.1G  4.0K  3.1G   1% /run/user/1000\n/dev/nvme1n1     3.5T   25G  3.4T   1% /mnt/nvme1n1\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"5",children:["\n",(0,t.jsx)(n.li,{children:"Update permissions"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd /mnt/nvme1n1\nsudo chmod 777 .\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"6",children:["\n",(0,t.jsx)(n.li,{children:"Configure AWS CLI"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sudo apt-get install awscli -y\naws configure\n"})}),"\n",(0,t.jsx)(n.h2,{id:"making-the-dataset",children:"Making the dataset"}),"\n",(0,t.jsxs)(n.p,{children:["Make a ",(0,t.jsx)(n.code,{children:"data/"})," directory to store the data, then go to ",(0,t.jsx)(n.a,{href:"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page",children:"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"})," and download the data.\nI used the 2011-2017 yellow taxi trip parquet files.\nNext, you need to convert the parquet files to TSV."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"cd data\npython -m venv taxis\nsource taxis/bin/activate\npip install pandas pyarrow\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Make a file ",(0,t.jsx)(n.code,{children:"parquet_to_tsv.py"})," with the following content."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"import pandas as pd\nimport glob\nimport os\nimport sys\n\ndef convert_parquet_to_tsv(input_directory, output_directory):\n    if not os.path.exists(output_directory):\n        os.makedirs(output_directory)\n\n    for parquet_file in glob.glob(os.path.join(input_directory, '*.parquet')):\n        df = pd.read_parquet(parquet_file)\n        base_name = os.path.basename(parquet_file)\n        tsv_file = os.path.join(output_directory, base_name.replace('.parquet', '.tsv'))\n        df.to_csv(tsv_file, sep='\\t', index=False)\n        print(f\"Converted {parquet_file} to {tsv_file}\")\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 3:\n        print(\"Usage: python script.py <input_directory> <output_directory>\")\n        sys.exit(1)\n\n    input_dir = sys.argv[1]\n    output_dir = sys.argv[2]\n    convert_parquet_to_tsv(input_dir, output_dir)\n"})}),"\n",(0,t.jsx)(n.p,{children:"Now run the conversion with"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"python parquet_to_tsv.py . .\n"})}),"\n",(0,t.jsx)(n.p,{children:"ClickHouse will use the TSV files, but for SigLens we'll use JSON."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'for year in {2011..2017}; do\n    for month in {01..12}; do\n        basefile="yellow_tripdata_$year-$month"\n        go run siglens/tools/sigclient/cmd/utils/converter.go --input "$basefile.tsv" --output "$basefile.json" &\n    done\ndone\nwait\n'})}),"\n",(0,t.jsx)(n.p,{children:"Finally, compress the TSV and JSON files with gzip and upload them to your AWS S3 bucket.\nI ingested the TSV and JSON files into separte directories to make it easier to download all of one type."}),"\n",(0,t.jsx)(n.h2,{id:"benchmark-siglens",children:"Benchmark SigLens"}),"\n",(0,t.jsx)(n.p,{children:"You'll want three terminals. Terminal 1 will run SigLens, Terminal 2 will do some setup and view the logs, and Terminal 3 will send the queries. Terminal 3 can run in your local machine if you setup the server to accept HTTP traffic, but Terminals 1 and 2 should be on the server. Start with Terminal 1."}),"\n",(0,t.jsx)(n.h3,{id:"install-go",children:"Install Go"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sudo apt update\nsudo apt install golang -y\n"})}),"\n",(0,t.jsx)(n.p,{children:"If prompted to restart some daemons, you can restart the recommended daemons."}),"\n",(0,t.jsx)(n.h3,{id:"clone-siglens",children:"Clone SigLens"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/siglens/siglens.git\ncd siglens\n"})}),"\n",(0,t.jsx)(n.h3,{id:"enable-agileaggs",children:"Enable AgileAggs"}),"\n",(0,t.jsxs)(n.p,{children:["Open ",(0,t.jsx)(n.code,{children:"server.yaml"})," and add these settings:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"agileAggsEnabled: true\npqsEnabled: true\n"})}),"\n",(0,t.jsx)(n.h3,{id:"start-siglens",children:"Start SigLens"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sudo go run cmd/siglens/main.go --config server.yaml\n"})}),"\n",(0,t.jsx)(n.p,{children:"Wait until SigLens is running. You'll see these lines in the terminal once it's up:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"INFO[2023-12-06 18:10:38] Extracting config from configFile: server.yaml\nINFO[2023-12-06 18:10:38] Defaulting to 2160hrs (90 days) of retention...\nINFO[2023-12-06 18:10:38] ----- Siglens server type SingleNode starting up -----\nINFO[2023-12-06 18:10:38] ----- Siglens Ingestion server starting on 0.0.0.0:8081 -----\nINFO[2023-12-06 18:10:38] ----- Siglens Query server starting on 0.0.0.0:5122 -----\nINFO[2023-12-06 18:10:38] ----- Siglens UI starting on 0.0.0.0:5122 -----\n"})}),"\n",(0,t.jsx)(n.h3,{id:"setup-pqs",children:"Setup PQS"}),"\n",(0,t.jsx)(n.p,{children:"Switch to Terminal 2 and run the following:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl -X POST -d \'{\n    "tableName": "trips",\n    "groupByColumns": ["airport_fee", "passenger_count", "PULocationID", "trip_distance"],\n    "measureColumns": ["total_amount"]\n}\' http://localhost:5122/api/pqs/aggs\necho ""\n'})}),"\n",(0,t.jsx)(n.p,{children:"You should get this response:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{"message":"All OK","status":200}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"setup-an-ingestion-script",children:"Setup an ingestion script"}),"\n",(0,t.jsxs)(n.p,{children:["In Terminal 2 run ",(0,t.jsx)(n.code,{children:"cd /mnt/nvme1n1/siglens/tools/sigclient"})," and then save the following into ingester.py"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import subprocess\nimport sys\n\n\ndef ingest(filename, batch_size=100):\n    # Determine the total number of lines in the file\n    total_lines = sum(1 for _ in open(filename, "r"))\n\n    lines = []\n    with open(filename, \'r\') as f:\n        for i, line in enumerate(f):\n            lines.append(line)\n\n            if len(lines) >= batch_size:\n                print(f"\\rProcessing... {((i + 1) / total_lines) * 100:.2f}%", end=\'\')\n                ingest_lines(lines)\n                lines = []\n    if lines:\n        ingest_lines(lines)\n        print(f"\\rProcessing... 100.00%")\n\n\ndef ingest_lines(lines):\n    index_data = \'{"index": {"_index": "trips", "_type": "_doc"}}\'\n    data = \'\'\n    for line in lines:\n        data += index_data + \'\\n\' + line\n\n    # Prepare the curl command\n    curl_command = [\n        "curl",\n        "-s",\n        "-o", "/dev/null",\n        "http://localhost:8081/elastic/_bulk",\n        "-X", "POST",\n        "-H", "Authorization: Bearer ",\n        "-H", "Content-Type: application/json",\n        "--data-binary", data\n    ]\n\n    # Execute the curl command\n    process = subprocess.run(curl_command, capture_output=False, text=False)\n    if process.stderr:\n        print("Error:", process.stderr)\n\n\nif __name__ == "__main__":\n    ingest(sys.argv[1])\n'})}),"\n",(0,t.jsx)(n.h3,{id:"ingest-the-data-into-siglens",children:"Ingest the data into SigLens"}),"\n",(0,t.jsx)(n.p,{children:"Make a dataset directory inside sigclient."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"mkdir dataset\n"})}),"\n",(0,t.jsx)(n.p,{children:"Run the following script to download, decompress, and ingest the data into SigLens."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'for year in {2011..2017}; do\n    for month in {01..12}; do\n        {\n            basefile="yellow_tripdata_$year-$month"\n\n            aws s3 cp s3://your-bucket/nyc-taxi-benchmark-data/json/$basefile.json.gz dataset/\n            gunzip dataset/$basefile.json.gz\n            python3 ingester.py dataset/$basefile.json\n        } &\n    done\n    wait\ndone\n'})}),"\n",(0,t.jsx)(n.h3,{id:"restart-siglens",children:"Restart SigLens"}),"\n",(0,t.jsx)(n.p,{children:"This step is to ensure that SigLens flushes all the ingested data. Simply Ctrl-C the process in Terminal 1 and restart it with"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sudo go run cmd/siglens/main.go --config server.yaml\n"})}),"\n",(0,t.jsx)(n.h3,{id:"view-logs",children:"View Logs"}),"\n",(0,t.jsx)(n.p,{children:"In terminal 2, run:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd /mnt/nvme1n1/siglens\nsudo tail -f siglens.log\n"})}),"\n",(0,t.jsx)(n.h3,{id:"run-the-queries-in-siglens",children:"Run the Queries in SigLens"}),"\n",(0,t.jsxs)(n.p,{children:["Run the following in Terminal 3.\nIf Terminal 3 is on your local machine, make sure to replace ",(0,t.jsx)(n.code,{children:"localhost"})," with the IP of the server.\nYou can remove the ",(0,t.jsx)(n.code,{children:" | python3 -m json.tool"})," if you want, it just formats the JSON response.\nCheck the log file ",(0,t.jsx)(n.code,{children:"siglens/siglens.log"})," for the query times."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl -X POST -d \'{\n    "searchText": "SELECT airport_fee, count(*) FROM trips GROUP BY airport_fee",\n    "index": "trips",\n    "startEpoch": "now-24h",\n    "endEpoch": "now",\n    "queryLanguage": "SQL"\n}\' http://localhost:5122/api/search | python3 -m json.tool\n\ncurl -X POST -d \'{\n    "searchText": "SELECT passenger_count, avg(total_amount) FROM trips GROUP BY passenger_count",\n    "index": "trips",\n    "startEpoch": "now-24h",\n    "endEpoch": "now",\n    "queryLanguage": "SQL"\n}\' http://localhost:5122/api/search | python3 -m json.tool\n\ncurl -X POST -d \'{\n    "searchText": "SELECT passenger_count, PULocationID, count(*) FROM trips GROUP BY passenger_count, PULocationID",\n    "index": "trips",\n    "startEpoch": "now-24h",\n    "endEpoch": "now",\n    "queryLanguage": "SQL"\n}\' http://localhost:5122/api/search | python3 -m json.tool\n\ncurl -X POST -d \'{\n    "searchText": "SELECT passenger_count, PULocationID, trip_distance, count(*) FROM trips GROUP BY passenger_count, PULocationID, trip_distance",\n    "index": "trips",\n    "startEpoch": "now-24h",\n    "endEpoch": "now",\n    "queryLanguage": "SQL"\n}\' http://localhost:5122/api/search | python3 -m json.tool\n'})}),"\n",(0,t.jsx)(n.h2,{id:"benchmark-clickhouse",children:"Benchmark ClickHouse"}),"\n",(0,t.jsx)(n.h3,{id:"install-clickhouse",children:"Install ClickHouse"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Prepare to install ClickHouse\nsudo apt-get install -y apt-transport-https ca-certificates dirmngr\nGNUPGHOME=$(mktemp -d)\nsudo GNUPGHOME="$GNUPGHOME" gpg --no-default-keyring --keyring /usr/share/keyrings/clickhouse-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 8919F6BD2B48D754\nsudo rm -r "$GNUPGHOME"\nsudo chmod +r /usr/share/keyrings/clickhouse-keyring.gpg\n\necho "deb [signed-by=/usr/share/keyrings/clickhouse-keyring.gpg] https://packages.clickhouse.com/deb stable main" | sudo tee \\\n    /etc/apt/sources.list.d/clickhouse.list\nsudo apt-get update\n\n# Install ClickHouse server and client\nsudo apt-get install -y clickhouse-server clickhouse-client\n'})}),"\n",(0,t.jsxs)(n.p,{children:["You should get the prompt ",(0,t.jsx)(n.code,{children:"Enter password for default user:"}),".\nEither create a pasword or just press enter to have no password."]}),"\n",(0,t.jsx)(n.h3,{id:"configure-the-clickhouse-data-folder",children:"Configure the ClickHouse data folder"}),"\n",(0,t.jsx)(n.p,{children:"This is an optional step to specify where ClickHouse should store its data.\nI did this during my testing so that both ClickHouse and SigLens would use the 3.5 TB storage space."}),"\n",(0,t.jsxs)(n.p,{children:["Use ",(0,t.jsx)(n.code,{children:"sudo vim /etc/clickhouse-server/config.xml"})," to change the line\n",(0,t.jsx)(n.code,{children:"<path>/var/lib/clickhouse/</path>"})," to ",(0,t.jsx)(n.code,{children:"<path>/mnt/nvme1n1/clickhouse/</path>"})]}),"\n",(0,t.jsx)(n.h3,{id:"run-clickhouse",children:"Run ClickHouse"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"sudo service clickhouse-server start\nclickhouse-client\n"})}),"\n",(0,t.jsx)(n.h3,{id:"make-the-clickhouse-table",children:"Make the ClickHouse Table"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"CREATE TABLE trips (\n    VendorID Int32,\n    tpep_pickup_datetime DateTime,\n    tpep_dropoff_datetime DateTime,\n    passenger_count Int32,\n    trip_distance Float32,\n    RatecodeID Int32,\n    store_and_fwd_flag FixedString(1),\n    PULocationID Int32,\n    DOLocationID Int32,\n    payment_type FixedString(3),\n    fare_amount Float32,\n    extra Float32,\n    mta_tax Float32,\n    tip_amount Float32,\n    tolls_amount Float32,\n    improvement_surcharge Float32,\n    total_amount Float32,\n    congestion_surcharge Float32,\n    airport_fee Float32)\nENGINE = MergeTree()\nORDER BY (tpep_pickup_datetime)\nSETTINGS index_granularity=8192\n"})}),"\n",(0,t.jsx)(n.h3,{id:"ingest-the-data",children:"Ingest the Data"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"INSERT INTO trips\nSELECT\n    VendorID,\n    tpep_pickup_datetime,\n    tpep_dropoff_datetime,\n    passenger_count,\n    trip_distance,\n    RatecodeID,\n    store_and_fwd_flag,\n    PULocationID,\n    DOLocationID,\n    payment_type,\n    fare_amount,\n    extra,\n    mta_tax,\n    tip_amount,\n    tolls_amount,\n    improvement_surcharge,\n    total_amount,\n    congestion_surcharge,\n    airport_fee\nFROM s3(\n    's3://your-bucket/nyc-taxi-benchmark-data/tsv/yellow_tripdata_{2011..2017}-{01..12}.tsv.gz',\n    'your_aws_access_key_id',\n    'your_aws_secret_access_key',\n    'TabSeparatedWithNames'\n);\n"})}),"\n",(0,t.jsx)(n.h3,{id:"run-the-queries-in-clickhouse",children:"Run the Queries in ClickHouse"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"# Query 1\nSELECT airport_fee, count(*) FROM trips GROUP BY airport_fee\n\n# Query 2\nSELECT passenger_count, avg(total_amount) FROM trips GROUP BY passenger_count\n\n# Query 3\nSELECT passenger_count, PULocationID, count(*) FROM trips GROUP BY passenger_count, PULocationID\n\n# Query 4\nSELECT passenger_count, PULocationID, trip_distance, count(*)\nFROM trips\nGROUP BY passenger_count, PULocationID, trip_distance\n"})}),"\n",(0,t.jsx)(n.h2,{id:"benchmark-elasticsearch",children:"Benchmark Elasticsearch"}),"\n",(0,t.jsx)(n.p,{children:"You'll want two terminals in your Elasticsearch server; the first will run Elasticsearch and the second will ingest data.\nStart with Terminal 1."}),"\n",(0,t.jsx)(n.h3,{id:"download-elasticsearch",children:"Download Elasticsearch"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd /mnt/nvme1n1\nwget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-8.11.4-linux-aarch64.tar.gz\nwget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-8.11.4-linux-aarch64.tar.gz.sha512\nshasum -a 512 -c elasticsearch-8.11.4-linux-aarch64.tar.gz.sha512 \n"})}),"\n",(0,t.jsxs)(n.p,{children:["This should output ",(0,t.jsx)(n.code,{children:"elasticsearch-8.11.4-linux-aarch64.tar.gz: OK"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"tar -xzf elasticsearch-8.11.4-linux-aarch64.tar.gz\ncd elasticsearch-8.11.4/\n"})}),"\n",(0,t.jsx)(n.h3,{id:"configure-elasticsearch",children:"Configure Elasticsearch"}),"\n",(0,t.jsxs)(n.p,{children:["Add the following to ",(0,t.jsx)(n.code,{children:"config/elasticsearch.yml"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"network.host: 0.0.0.0\ndiscovery.type: single-node\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Also delete the line ",(0,t.jsx)(n.code,{children:'cluster.initial_master_nodes: ["ip-172-31-24-1"]'})," from that file."]}),"\n",(0,t.jsx)(n.p,{children:"Set a custom heap size. On this machine, if I increased the heap limit past 24 GB, ingestion would crash Elasticsearch."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'echo -e "-Xms24g\\n-Xmx24g" > config/jvm.options.d/heap.options\n'})}),"\n",(0,t.jsxs)(n.p,{children:["Elasticsearch uses mmapfs for storing indices. You can check the limit with ",(0,t.jsx)(n.code,{children:"sudo sysctl vm.max_map_count"}),".\nI got 65530 and will increase this with"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"sudo sysctl -w vm.max_map_count=262144\n"})}),"\n",(0,t.jsx)(n.h3,{id:"add-a-template",children:"Add a template"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl -k -u "elastic:<your-elastic-password>" --location --request PUT \'https://<server-ip>:9200/_template/temp1\' \\\n    --header \'Content-Type: application/json\' \\\n    --data-raw \'{\n        "index_patterns": "trips",\n        "settings": { "number_of_shards": 6, "number_of_replicas": 0 },\n        "mappings": {\n            "_source": { "enabled": true },\n            "properties": {\n                "timestamp": { "type": "date", "format": "epoch_millis" }\n            }\n        }\n    }\'\n'})}),"\n",(0,t.jsx)(n.h3,{id:"save-the-password-for-later",children:"Save the password for later"}),"\n",(0,t.jsxs)(n.p,{children:["If you don't know the password for the ",(0,t.jsx)(n.code,{children:"elastic"})," user, reset it with"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"bin/elasticsearch-reset-password -u elastic\n"})}),"\n",(0,t.jsx)(n.p,{children:"The new password will be printed. Save it for later."}),"\n",(0,t.jsx)(n.h3,{id:"run-elasticsearch",children:"Run Elasticsearch"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"./bin/elasticsearch > elastic.log 2>&1\n"})}),"\n",(0,t.jsx)(n.h3,{id:"ingest-the-data-1",children:"Ingest the Data"}),"\n",(0,t.jsx)(n.p,{children:"In Terminal 2, clone siglens to use an ingester script from it."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd /mnt/nvme1n1\ngit clone https://github.com/siglens/siglens.git\ncd siglens/tools/nyc-taxi-benchmark\n"})}),"\n",(0,t.jsx)(n.p,{children:"In ingester.py, make these changes:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Change ",(0,t.jsx)(n.code,{children:'index_data = \'{"index": {"_index": "trips", "_type": "_doc"}}\''})," to ",(0,t.jsx)(n.code,{children:'index_data = \'{"index": {"_index": "trips"}}\''})]}),"\n",(0,t.jsxs)(n.li,{children:["In the curl section, add the ",(0,t.jsx)(n.code,{children:'"-k",'})," option, which allows faking SSL checks"]}),"\n",(0,t.jsxs)(n.li,{children:["In the curl section, add ",(0,t.jsx)(n.code,{children:'"-u", "elastic:<your-elastic-password>",'})]}),"\n",(0,t.jsxs)(n.li,{children:["In the curl section, change ",(0,t.jsx)(n.code,{children:'"http://localhost:8081/elastic/_bulk",'})," to ",(0,t.jsx)(n.code,{children:'"https://localhost:9200/elastic/_bulk",'})]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Now download and ingest the data:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'mkdir dataset\n\nfor year in {2011..2017}; do\n    for month in {01..12}; do\n        {\n            basefile="yellow_tripdata_$year-$month"\n\n            aws s3 cp s3://siglens-benchmark-datasets/nyc-taxi-benchmark-data/json/$basefile.json.gz dataset/\n            gunzip dataset/$basefile.json.gz\n            python3 ingester.py dataset/$basefile.json\n        } &\n    done\n    wait\ndone\n'})}),"\n",(0,t.jsx)(n.h3,{id:"prepare-to-run-queries",children:"Prepare to run queries"}),"\n",(0,t.jsxs)(n.p,{children:["This is an optional step so that we know what to set the ",(0,t.jsx)(n.code,{children:"size"})," parameters to in our queries with group by fields.\nHowever, you can skip this because the queries in the next section already have the correct ",(0,t.jsx)(n.code,{children:"size"})," values."]}),"\n",(0,t.jsx)(n.p,{children:"The following should indicate 36 unique values."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl -k -u "elastic:<your-elastic-password>" "https://<server-ip>:9200/trips/_search" -H \'Content-Type: application/json\' -d \'{\n  "size": 0,\n  "aggs": {\n    "distinct_passenger_count": {\n      "cardinality": {\n        "field": "passenger_count"\n      }\n    }\n  }\n}\' | python3 -m json.tool\n'})}),"\n",(0,t.jsx)(n.p,{children:"The following should indicate 265 unique values."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl -k -u "elastic:<your-elastic-password>" "https://<server-ip>:9200/trips/_search" -H \'Content-Type: application/json\' -d \'{\n  "size": 0,\n  "aggs": {\n    "distinct_PULocationID": {\n      "cardinality": {\n        "field": "PULocationID"\n      }\n    }\n  }\n}\' | python3 -m json.tool\n'})}),"\n",(0,t.jsx)(n.p,{children:"The following should indicate 11473 unique values."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl -k -u "elastic:<your-elastic-password>" "https://<server-ip>:9200/trips/_search" -H \'Content-Type: application/json\' -d \'{\n  "size": 0,\n  "aggs": {\n    "distinct_trip_distance": {\n      "cardinality": {\n        "field": "trip_distance"\n      }\n    }\n  }\n}\' | python3 -m json.tool\n'})}),"\n",(0,t.jsx)(n.h3,{id:"run-the-queries-in-elasticsearch",children:"Run the Queries in Elasticsearch"}),"\n",(0,t.jsx)(n.p,{children:"To get benchmark results, I had to clear the Elasticsearch cache after every query.\nOtherwise, if I ran a query multiple times then the first time would take a while but every subsequent invocation would return much faster than the original search because it was returning the cached response.\nTo clear the cache, run:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'curl -k -u "elastic:<your-elastic-password>" -X POST "https://<server-ip>:9200/trips/_cache/clear"\n'})}),"\n",(0,t.jsxs)(n.p,{children:["The responses will have a ",(0,t.jsx)(n.code,{children:"took"})," field, indicating how long the query took in milliseconds."]}),"\n",(0,t.jsxs)(n.p,{children:["Note that Query 1 is a little different than Query 1 for the other benchmarked databases.\nThis is because Elasticsearch was unable to perform an aggregation on the ",(0,t.jsx)(n.code,{children:"airport_fee"})," column because it was ingested as a text field.\nSo instead, this Query 1 aggregates on the ",(0,t.jsx)(n.code,{children:"improvement_surcharge"})," field.\nThis should be comparable because the ",(0,t.jsx)(n.code,{children:"airport_fee"})," column only has one bucket, while the ",(0,t.jsx)(n.code,{children:"improvement_surcharge"})," column has only 2, and one of those only accounts for 360 rows of the more than 1 billion rows in the dataset."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Query 1\nSELECT airport_fee, count(*) FROM trips GROUP BY airport_fee\ncurl -k -u "elastic:<your-elastic-password>" "https://<server-ip>:9200/trips/_search" -H \'Content-Type: application/json\' -d \'{\n  "size": 0,\n  "aggs": {\n    "improvement_surcharge_groups": {\n      "terms": {\n        "field": "improvement_surcharge",\n        "size": 10\n      },\n      "aggs": {\n        "count": {\n          "value_count": {\n            "field": "improvement_surcharge"\n          }\n        }\n      }\n    }\n  }\n}\' | python3 -m json.tool | less\n\n# Query 2\ncurl -k -u "elastic:<your-elastic-password>" "https://<server-ip>:9200/trips/_search" -H \'Content-Type: application/json\' -d \'{\n  "size": 0,\n  "aggs": {\n    "passenger_count_groups": {\n      "terms": {\n        "field": "passenger_count",\n        "size": 36\n      },\n      "aggs": {\n        "average_total_amount": {\n          "avg": {\n            "field": "total_amount"\n          }\n        }\n      }\n    }\n  }\n}\' | python3 -m json.tool | less\n\n# Query 3\ncurl -k -u "elastic:<your-elastic-password>" "https://<server-ip>:9200/trips/_search" -H \'Content-Type: application/json\' -d \'{\n  "size": 0,\n  "aggs": {\n    "passenger_count_groups": {\n      "terms": {\n        "field": "passenger_count",\n        "size": 36\n      },\n      "aggs": {\n        "PULocationID_groups": {\n          "terms": {\n            "field": "PULocationID",\n            "size": 265\n          }\n        }\n      }\n    }\n  }\n}\' | python3 -m json.tool | less\n\n# Query 4\n# For this query we don\'t want more than 10,000 buckets, so we\'ll reduce the "size" parameters.\ncurl -k -u "elastic:<your-elastic-password>" "https://<server-ip>:9200/trips/_search" -H \'Content-Type: application/json\' -d \'{\n  "size": 0,\n  "aggs": {\n    "passenger_count_groups": {\n      "terms": {\n        "field": "passenger_count",\n        "size": 10\n      },\n      "aggs": {\n        "PULocationID_groups": {\n          "terms": {\n            "field": "PULocationID",\n            "size": 10\n          },\n          "aggs": {\n            "trip_distance_groups": {\n              "terms": {\n                "field": "trip_distance",\n                "size": 100\n              },\n              "aggs": {\n                "count": {\n                  "value_count": {\n                    "field": "trip_distance"\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\' | python3 -m json.tool | less\n'})}),"\n",(0,t.jsx)(n.h2,{id:"benchmark-loki",children:"Benchmark Loki"}),"\n",(0,t.jsx)(n.p,{children:"You'll want three terminals. Terminal 1 will run Loki, Terminal 2 will run Promtail, and Terminal 3 will run ingestion and queries."}),"\n",(0,t.jsx)(n.h3,{id:"install-loki-and-promtail",children:"Install Loki and Promtail"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'sudo apt install unzip -y\n\nmkdir /mnt/nvme1n1/loki\ncd /mnt/nvme1n1/loki\n\ncurl -O -L "https://github.com/grafana/loki/releases/download/v2.9.4/loki-linux-arm64.zip"\nunzip loki-linux-arm64.zip\nchmod a+x loki-linux-arm64\n\ncurl -O -L "https://github.com/grafana/loki/releases/download/v2.9.4/promtail-linux-arm64.zip"\nunzip promtail-linux-arm64.zip\nchmod a+x promtail-linux-arm64\n\nwget https://raw.githubusercontent.com/grafana/loki/v2.9.4/cmd/loki/loki-local-config.yaml\nwget https://raw.githubusercontent.com/grafana/loki/v2.9.4/clients/cmd/promtail/promtail-local-config.yaml\n'})}),"\n",(0,t.jsx)(n.h3,{id:"update-the-default-configs",children:"Update the default configs"}),"\n",(0,t.jsxs)(n.p,{children:["In ",(0,t.jsx)(n.code,{children:"loki-local-config.yaml"}),", update these configurations with these new values:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"path_prefix: /mnt/nvme1n1/loki/internal/path\nchunks_directory: /mnt/nvme1n1/loki/internal/chunks\nrules_directory: /mnt/nvme1n1/loki/internal/rules\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Increase the timeout limit by adding these into the ",(0,t.jsx)(n.code,{children:"server"})," section:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"http_server_read_timeout: 12h\nhttp_server_write_timeout: 12h\n"})}),"\n",(0,t.jsx)(n.p,{children:"Add this to your config to avoid ingestion throttling and increase the number of buckets that queries are allowed to return."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"limits_config:\n  per_stream_rate_limit: 4G\n  per_stream_rate_limit_burst: 4G\n  ingestion_rate_mb: 4096\n  ingestion_burst_size_mb: 4096\n  max_query_series: 1000000\n  query_timeout: 12h\n"})}),"\n",(0,t.jsxs)(n.p,{children:["In ",(0,t.jsx)(n.code,{children:"promtail-local-config.yaml"}),", change ",(0,t.jsx)(n.code,{children:"__path__"})," to:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"__path__: /mnt/nvme1n1/data/*.json\n"})}),"\n",(0,t.jsx)(n.h3,{id:"run-loki-and-promtail",children:"Run Loki and Promtail"}),"\n",(0,t.jsx)(n.p,{children:"In Terminal 1, start Loki with:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"./loki-linux-arm64 -config.file=loki-local-config.yaml\n"})}),"\n",(0,t.jsx)(n.p,{children:"In Terminal 2, start Promtail with:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"./promtail-linux-arm64 -config.file=promtail-local-config.yaml\n"})}),"\n",(0,t.jsx)(n.h3,{id:"ingest-the-data-2",children:"Ingest the Data"}),"\n",(0,t.jsx)(n.p,{children:"In Terminal 3, run:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'mkdir /mnt/nvme1n1/data\ncd /mnt/nvme1n1/data\n\nfor year in {2011..2017}; do\n    for month in {01..12}; do\n            basefile="yellow_tripdata_$year-$month"\n\n            aws s3 cp s3://siglens-benchmark-datasets/nyc-taxi-benchmark-data/json/$basefile.json.gz .\n            gunzip $basefile.json.gz\n    done\ndone\n'})}),"\n",(0,t.jsxs)(n.p,{children:["Since Promtail is already running and configured to ingest all JSON files in this ",(0,t.jsx)(n.code,{children:"data/"})," directory, it will start ingesting the logs.\nYou can check the progress of the ingestion with this bash:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"total=$(curl -s http://localhost:9080/metrics | grep -v '^#' | awk '/promtail_file_bytes_total/ {sum += $2} END {print sum}'); \\\ningested=$(curl -s http://localhost:9080/metrics | grep -v '^#' | awk '/promtail_read_bytes_total/ {sum += $2} END {print sum}'); \\\npercent=$(echo \"$ingested $total\" | awk '{printf \"%.2f\\n\", ($1 / $2) * 100}'); \\\necho \"percent ingested: $percent%\"\n"})}),"\n",(0,t.jsx)(n.h3,{id:"run-the-queries-in-loki",children:"Run the Queries in Loki"}),"\n",(0,t.jsx)(n.p,{children:"After ingestion is complete, run the queries."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Query 1\ncurl -G -s "http://localhost:3100/loki/api/v1/query" \\\n  --data-urlencode "query=sum(count_over_time({job=\\"varlogs\\"} | json [24h])) by (airport_fee)" \\\n  --data-urlencode "time=$(date +%s)000000000" \\\n  --data-urlencode "stats=true"\n\n# Query 2\ncurl -G -s "http://localhost:3100/loki/api/v1/query" \\\n  --data-urlencode "query=avg_over_time({job=\\"varlogs\\"} | json | unwrap total_amount [24h]) by (passenger_count)" \\\n  --data-urlencode "time=$(date +%s)000000000" \\\n  --data-urlencode "stats=true"\n\n# Query 3\ncurl -G -s "http://localhost:3100/loki/api/v1/query" \\\n  --data-urlencode "query=sum(count_over_time({job=\\"varlogs\\"} | json [24h])) by (passenger_count, PULocationID)" \\\n  --data-urlencode "time=$(date +%s)000000000" \\\n  --data-urlencode "stats=true"\n\n# Query 4\ncurl -G -s "http://localhost:3100/loki/api/v1/query" \\\n  --data-urlencode "query=sum(count_over_time({job=\\"varlogs\\"} | json [24h])) by (passenger_count, PULocationID, trip_distance)" \\\n  --data-urlencode "time=$(date +%s)000000000" \\\n  --data-urlencode "stats=true"\n'})})]})}function h(e={}){const{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},1151:(e,n,s)=>{s.d(n,{Z:()=>l,a:()=>r});var t=s(7294);const a={},i=t.createContext(a);function r(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);